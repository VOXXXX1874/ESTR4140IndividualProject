{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.models import resnet18\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from torch.optim import lr_scheduler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import ttach as tta\n",
    "from typing import Callable, List, Tuple, Optional\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.activations_and_gradients import ActivationsAndGradients\n",
    "from pytorch_grad_cam.utils.svd_on_activations import get_2d_projection\n",
    "from pytorch_grad_cam.utils.image import scale_cam_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import (\n",
    "    show_cam_on_image, deprocess_image, preprocess_image\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyImageFolder(datasets.ImageFolder):  \n",
    "    def __init__(self, root, transform=None, target_transform=None,  \n",
    "                 loader=datasets.folder.default_loader,  \n",
    "                 is_valid_file=None,):  \n",
    "        super(MyImageFolder, self).__init__(root, transform, target_transform, loader, is_valid_file)  \n",
    "\n",
    "    def __getitem__(self, index): \n",
    "        path, _ = self.samples[index] \n",
    "        sample, _ = super(MyImageFolder, self).__getitem__(index) \n",
    "        return sample, path[-16:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "train_transform_gpu = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "train_ds = MyImageFolder('./task_images_downsampled/train',\n",
    "                                transform = train_transform,)\n",
    "batch_size=64\n",
    "def get_dataloader_workers():\n",
    "    return 4\n",
    "train_dl = DataLoader(train_ds,batch_size=batch_size,shuffle=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseCAM:\n",
    "    def __init__(self,\n",
    "                 model: torch.nn.Module,\n",
    "                 target_layers: List[torch.nn.Module],\n",
    "                 reshape_transform: Callable = None,\n",
    "                 compute_input_gradient: bool = False,\n",
    "                 uses_gradients: bool = True,\n",
    "                 tta_transforms: Optional[tta.Compose] = None) -> None:\n",
    "        self.model = model.eval()\n",
    "        self.target_layers = target_layers\n",
    "\n",
    "        # Use the same device as the model.\n",
    "        self.device = next(self.model.parameters()).device\n",
    "        self.reshape_transform = reshape_transform\n",
    "        self.compute_input_gradient = compute_input_gradient\n",
    "        self.uses_gradients = uses_gradients\n",
    "        if tta_transforms is None:\n",
    "            self.tta_transforms = tta.Compose(\n",
    "                [\n",
    "                    tta.HorizontalFlip(),\n",
    "                    tta.Multiply(factors=[0.9, 1, 1.1]),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            self.tta_transforms = tta_transforms\n",
    "\n",
    "        self.activations_and_grads = ActivationsAndGradients(\n",
    "            self.model, target_layers, reshape_transform)\n",
    "\n",
    "    \"\"\" Get a vector of weights for every channel in the target layer.\n",
    "        Methods that return weights channels,\n",
    "        will typically need to only implement this function. \"\"\"\n",
    "\n",
    "    def get_cam_weights(self,\n",
    "                        input_tensor: torch.Tensor,\n",
    "                        target_layers: List[torch.nn.Module],\n",
    "                        targets: List[torch.nn.Module],\n",
    "                        activations: torch.Tensor,\n",
    "                        grads: torch.Tensor) -> np.ndarray:\n",
    "        raise Exception(\"Not Implemented\")\n",
    "\n",
    "\n",
    "    def forward(self,\n",
    "                input_tensor: torch.Tensor,\n",
    "                targets: List[torch.nn.Module],\n",
    "                eigen_smooth: bool = False) -> np.ndarray:\n",
    "\n",
    "        input_tensor = input_tensor.to(self.device)\n",
    "\n",
    "        if self.compute_input_gradient:\n",
    "            input_tensor = torch.autograd.Variable(input_tensor,\n",
    "                                                   requires_grad=True)\n",
    "\n",
    "        self.outputs = outputs = self.activations_and_grads(input_tensor)\n",
    "\n",
    "        if targets is None:\n",
    "            target_categories = np.argmax(outputs.cpu().data.numpy(), axis=-1)\n",
    "            targets = [ClassifierOutputTarget(\n",
    "                category) for category in target_categories]\n",
    "\n",
    "        if self.uses_gradients:\n",
    "            self.model.zero_grad()\n",
    "            loss = sum([target(output)\n",
    "                       for target, output in zip(targets, outputs)])\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "        # In most of the saliency attribution papers, the saliency is\n",
    "        # computed with a single target layer.\n",
    "        # Commonly it is the last convolutional layer.\n",
    "        # Here we support passing a list with multiple target layers.\n",
    "        # It will compute the saliency image for every image,\n",
    "        # and then aggregate them (with a default mean aggregation).\n",
    "        # This gives you more flexibility in case you just want to\n",
    "        # use all conv layers for example, all Batchnorm layers,\n",
    "        # or something else.\n",
    "        cam = self.compute_cam_per_layer(input_tensor,\n",
    "                                                   targets,\n",
    "                                                   eigen_smooth)\n",
    "        return cam\n",
    "\n",
    "    def compute_cam_per_layer(\n",
    "            self,\n",
    "            input_tensor: torch.Tensor,\n",
    "            targets: List[torch.nn.Module],\n",
    "            eigen_smooth: bool) -> np.ndarray:\n",
    "        activations = self.activations_and_grads.activations[0]\n",
    "        grads = self.activations_and_grads.gradients[0]\n",
    "        # Loop over the saliency image from every layer\n",
    "        cam = activations*grads\n",
    "        cam = torch.max(cam, torch.tensor(0.0))\n",
    "\n",
    "        return cam\n",
    "\n",
    "    def __call__(self,\n",
    "                 input_tensor: torch.Tensor,\n",
    "                 targets: List[torch.nn.Module] = None,\n",
    "                 aug_smooth: bool = False,\n",
    "                 eigen_smooth: bool = False) -> np.ndarray:\n",
    "\n",
    "        # Smooth the CAM result with test time augmentation\n",
    "        if aug_smooth is True:\n",
    "            return self.forward_augmentation_smoothing(\n",
    "                input_tensor, targets, eigen_smooth)\n",
    "\n",
    "        return self.forward(input_tensor,\n",
    "                            targets, eigen_smooth)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.activations_and_grads.release()\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, exc_tb):\n",
    "        self.activations_and_grads.release()\n",
    "        if isinstance(exc_value, IndexError):\n",
    "            # Handle IndexError here...\n",
    "            print(\n",
    "                f\"An exception occurred in CAM with block: {exc_type}. Message: {exc_value}\")\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCAM(BaseCAM):\n",
    "    def __init__(self, model, target_layers,\n",
    "                 reshape_transform=None):\n",
    "        super(\n",
    "            GradCAM,\n",
    "            self).__init__(\n",
    "            model,\n",
    "            target_layers,\n",
    "            reshape_transform)\n",
    "\n",
    "    def get_cam_weights(self,\n",
    "                        input_tensor,\n",
    "                        target_layer,\n",
    "                        target_category,\n",
    "                        activations,\n",
    "                        grads):\n",
    "        return torch.mean(grads, axis=(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = resnet18(num_classes=2).to(device).eval()\n",
    "model.load_state_dict(torch.load('best.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interprete(model,input_tensor,targets,path):\n",
    "    target_layers = [model.layer4[-1]]\n",
    "    cam_algorithm = GradCAM\n",
    "    with cam_algorithm(model=model,\n",
    "                        target_layers=target_layers) as cam:\n",
    "        cam.batch_size = 32\n",
    "\n",
    "        grayscale_cam = cam(input_tensor=input_tensor, \n",
    "                            targets=targets,\n",
    "                            )\n",
    "        grayscale_cam = grayscale_cam[0, :]\n",
    "        grayscale_cam = grayscale_cam/grayscale_cam.max()\n",
    "\n",
    "    grayscale_cam[grayscale_cam > 0.8] = -1\n",
    "    grayscale_cam[grayscale_cam > 0.6] = -0.8\n",
    "    grayscale_cam[grayscale_cam > 0.4] = -0.6\n",
    "    grayscale_cam[grayscale_cam > 0.2] = -0.4\n",
    "    grayscale_cam[grayscale_cam > 0] = -0.2\n",
    "    grayscale_cam_array = grayscale_cam.numpy()\n",
    "\n",
    "    np.savez_compressed(path, compressed_array=grayscale_cam_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 964/964 [13:37<00:00,  1.18it/s]\n"
     ]
    }
   ],
   "source": [
    "for X,name in tqdm(train_dl):\n",
    "    X = X.to(device)\n",
    "    for j in range(len(X)):\n",
    "        targets = None\n",
    "        input_tensor = train_transform_gpu(X[j:j+1]).to(device)\n",
    "        interprete(model,input_tensor,targets,\"./dropout_prob/\"+name[j][0:-4]+\".npz\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
